{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Encoding\n",
    "\n",
    "First, let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "import pandas as pd\n",
    "training_set = pd.read_csv(\"housing_price_data/training_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our columns are numeric, it's straightforward to add them to a linear regression model. Below we are adding an extra feature to a model `GrLivArea`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[['BedroomAbvGr','GrLivArea']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding binomial values\n",
    "\n",
    "In order to make use of categorical data, we first need to encode it as a number. As a simple example, we'll encode the variable `CentralAir` as 0 if there is no central air conditioning, and 1 if there is central air conditioning. We can do this by using a boolean comparison operation, and relying on the fact that `True == 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see the output of encoding CentralAir this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_set['CentralAir'] == 'Y').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding ordinal values\n",
    "If the categories of a variable follow a clear rank, then we can label them by this rank. An example of this is the basement quality column.\n",
    "\n",
    "    BsmtCond: Evaluates the general condition of the basement\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical - slight dampness allowed\n",
    "       Fa\tFair - dampness or some cracking or settling\n",
    "       Po\tPoor - Severe cracking, settling, or wetness\n",
    "\n",
    "We would encode this as Po:1, Fa:2, TA:3, Gd:4, and Ex:5.\n",
    "\n",
    "For houses without a basement (i.e. `BsmtCond is NaN`), we use a default value of 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode basement condition using one-hot-encoding\n",
    "bsmt_cond_map = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1}\n",
    "training_set['BsmtCond'].map(bsmt_cond_map).fillna(0).head() # Some houses have no basement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If there is no order, we use a technique called 'one hot encoding'. This involves creating a new column for each category\n",
    "\n",
    "For example, the `Electrical` variable in the housing prices dataset contains the following categories:\n",
    "\n",
    "       SBrkr\tStandard Circuit Breakers & Romex\n",
    "       FuseA\tFuse Box over 60 AMP and all Romex wiring (Average)\t\n",
    "       FuseF\t60 AMP Fuse Box and mostly Romex wiring (Fair)\n",
    "       FuseP\t60 AMP Fuse Box and mostly knob & tube wiring (poor)\n",
    "       Mix\tMixed\n",
    "\n",
    "We would encode the table\n",
    "\n",
    "|Electrical|\n",
    "|---------|\n",
    "  |FuseA|\n",
    "  |FuseF|\n",
    "  |FuseP|\n",
    "  |Mix|\n",
    "  |SBrkr|\n",
    "  \n",
    "as\n",
    "\n",
    "|FuseA |FuseF|FuseP|Mix|SBrkr|\n",
    "|-----|-----|-----|-----|\n",
    "| 1| 0  | 0   | 0  | 0  |\n",
    "| 0|1  | 0   | 0  | 0  |\n",
    "| 0|0  | 1  | 0| 0|\n",
    "| 0|0  | 0 | 1 |0 |\n",
    "| 0|0  | 0 | 0 | 1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_electrical(electrical):\n",
    "    one_hot_encoding = pd.DataFrame()\n",
    "    one_hot_encoding['FuseA'] = electrical == 'FuseA'\n",
    "    one_hot_encoding['FuseF'] = electrical == 'FuseF'\n",
    "    one_hot_encoding['FuseP'] = electrical == 'FuseP'\n",
    "    one_hot_encoding['Mix']   = electrical == 'Mix'\n",
    "    one_hot_encoding['SBrkr'] = (1 - one_hot_encoding.sum(axis=1)) # 'SBrkr' is standard default\n",
    "    \n",
    "    return(one_hot_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_electrical(training_set['Electrical']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, combing some features together will aid the machine learning process.  In this example we will multiple the number of bedrooms with the number of bathrooms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_and_bath = training_set['FullBath'] * training_set['BedroomAbvGr']\n",
    "bed_and_bath.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning algorithms require all variables to be on the same scale, ideally between -1 and 1. Let's compare LotArea to SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[['LotArea','SalePrice']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform these variables to be on the same scale using a preprocessing trick called _min/max scaling_. \n",
    "\n",
    "`MinMaxScale(X) = (X - min(X))/(max(X) - min(X))` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled = pd.DataFrame(scaler.fit_transform(training_set[['LotArea','SalePrice']]),\n",
    "             columns=['LotArea','SalePrice'])\n",
    "scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put these back to their original scale using the `inverse_transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(scaled),\n",
    "             columns=[\"LotArea\",\"SalePrice\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[[\"LotArea\",\"SalePrice\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add new features to our model by combining two or more existing features. For example, let's\n",
    "multiply bedrooms by bathrooms. \n",
    "We'll also scale our input and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Let's take what we've learnt above and create an encode_features function that encodes a number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_model\n",
    "from sklearn import linear_model\n",
    "\n",
    "def encode_features(data, scaler=None):\n",
    "    features = data.copy()\n",
    "    \n",
    "    # Encode Central Air where Y is 1, and N is 0\n",
    "    features['CentralAir'] = features['CentralAir'] == 'Y'\n",
    "\n",
    "    # Encode basement condition using one-hot-encoding\n",
    "    bsmt_cond_map = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1}\n",
    "    features['BsmtQuality'] = features['BsmtCond'].map(bsmt_cond_map).fillna(0) # Some houses have no basement\n",
    "\n",
    "    # Encode electrical\n",
    "    features = pd.concat([features, encode_electrical(features['Electrical'])], axis=1)\n",
    "    \n",
    "    # Combine bed and bath    \n",
    "    features['BedBath'] = features['FullBath'] * features['BedroomAbvGr']\n",
    "    \n",
    "    # Scale numeric values\n",
    "    scaled_columns = [\n",
    "        'FullBath',\n",
    "        'BedroomAbvGr', \n",
    "        'BedBath',\n",
    "        'GrLivArea',\n",
    "        'CentralAir', \n",
    "        'FuseA',\n",
    "        'FuseF',\n",
    "        'FuseP', \n",
    "        'Mix',\n",
    "        'SBrkr', \n",
    "        'BsmtQuality'\n",
    "    ]\n",
    "    scaled_features = features[scaled_columns]\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(scaled_features)\n",
    "        \n",
    "    scaled_features = pd.DataFrame(scaler.transform(scaled_features), columns = scaled_columns)\n",
    "    \n",
    "    return scaled_features, scaler\n",
    "\n",
    "def encode_label(data):\n",
    "    labels = data.copy()['SalePrice']\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(labels)    \n",
    "    labels = pd.DataFrame(scaler.transform(labels), columns = ['SalePrice'])\n",
    "    return (labels['SalePrice'], scaler)\n",
    "\n",
    "def decode_label(data, scaler):\n",
    "    return scaler.inverse_transform(data)\n",
    "\n",
    "def train_model(training_set):\n",
    "    features, feature_scaler = encode_features(training_set)\n",
    "    labels, label_scaler = encode_label(training_set)\n",
    "    predictor = linear_model.LinearRegression()\n",
    "    \n",
    "    predictor.fit(features,labels)\n",
    "    \n",
    "    def model(input_data):\n",
    "        input_features,_ = encode_features(input_data, scaler=feature_scaler)\n",
    "        output_value = predictor.predict(input_features)\n",
    "        return decode_label(output_value, label_scaler)\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,_ =encode_features(training_set)\n",
    "data.head()\n",
    "scaled_model = train_model(training_set)\n",
    "evaluate_model(scaled_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try some of the following:\n",
    "- add `LotArea` to the multilinear model and observe the performance of the model\n",
    "- add whether or not the house has a pool to this model in the code above. (Look at the `PoolArea` variable).\n",
    "- add to the code above the KitchenQuality as a feature (see `KitchenQual` in `data_description.txt`)\n",
    "- add `Heating` using One Hot encoding\n",
    "\n",
    "Compare your new models accuracy to the above. Did it improve it or make it worse? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
