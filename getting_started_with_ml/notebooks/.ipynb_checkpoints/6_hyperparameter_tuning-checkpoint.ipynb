{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore tuning the hyperparameters of the Support Vector Regression algorithms using SciKit Learn's Grid Search function [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/tim/opt/miniconda3/envs/training\n",
      "\n",
      "  added / updated specs:\n",
      "    - numpy\n",
      "    - pandas\n",
      "    - scikit-learn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    joblib-0.16.0              |             py_0         210 KB\n",
      "    mkl_fft-1.1.0              |   py37hc64f4ea_0         135 KB\n",
      "    numpy-1.18.5               |   py37h1da2735_0           5 KB\n",
      "    numpy-base-1.18.5          |   py37h3304bdc_0         3.9 MB\n",
      "    pandas-1.0.5               |   py37h959d312_0         7.4 MB\n",
      "    scikit-learn-0.23.1        |   py37h603561c_0         4.6 MB\n",
      "    scipy-1.5.0                |   py37h912ce22_0        13.2 MB\n",
      "    threadpoolctl-2.1.0        |     pyh5ca1d4c_0          17 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        29.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  blas               pkgs/main/osx-64::blas-1.0-mkl\n",
      "  intel-openmp       pkgs/main/osx-64::intel-openmp-2019.4-233\n",
      "  joblib             pkgs/main/noarch::joblib-0.16.0-py_0\n",
      "  libgfortran        pkgs/main/osx-64::libgfortran-3.0.1-h93005f0_2\n",
      "  llvm-openmp        pkgs/main/osx-64::llvm-openmp-10.0.0-h28b9765_0\n",
      "  mkl                pkgs/main/osx-64::mkl-2019.4-233\n",
      "  mkl-service        pkgs/main/osx-64::mkl-service-2.3.0-py37hfbe908c_0\n",
      "  mkl_fft            pkgs/main/osx-64::mkl_fft-1.1.0-py37hc64f4ea_0\n",
      "  mkl_random         pkgs/main/osx-64::mkl_random-1.1.1-py37h959d312_0\n",
      "  numpy              pkgs/main/osx-64::numpy-1.18.5-py37h1da2735_0\n",
      "  numpy-base         pkgs/main/osx-64::numpy-base-1.18.5-py37h3304bdc_0\n",
      "  pandas             pkgs/main/osx-64::pandas-1.0.5-py37h959d312_0\n",
      "  pytz               pkgs/main/noarch::pytz-2020.1-py_0\n",
      "  scikit-learn       pkgs/main/osx-64::scikit-learn-0.23.1-py37h603561c_0\n",
      "  scipy              pkgs/main/osx-64::scipy-1.5.0-py37h912ce22_0\n",
      "  threadpoolctl      pkgs/main/noarch::threadpoolctl-2.1.0-pyh5ca1d4c_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "mkl_fft-1.1.0        | 135 KB    | ##################################### | 100% \n",
      "pandas-1.0.5         | 7.4 MB    | ##################################### | 100% \n",
      "numpy-base-1.18.5    | 3.9 MB    | ##################################### | 100% \n",
      "scikit-learn-0.23.1  | 4.6 MB    | ##################################### | 100% \n",
      "numpy-1.18.5         | 5 KB      | ##################################### | 100% \n",
      "scipy-1.5.0          | 13.2 MB   | ##################################### | 100% \n",
      "threadpoolctl-2.1.0  | 17 KB     | ##################################### | 100% \n",
      "joblib-0.16.0        | 210 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn.svm\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same evaluate_model function as before\n",
    "def evaluate_model(model_fn, print_result=False):\n",
    "    '''\n",
    "    Consumes a function model_fn\n",
    "    and evaluates its predictive accuracy against \n",
    "    the housing prices test set.\n",
    "    We have included a switch for the output to be a more human readable\n",
    "    printed version or the uncurtailed floating point value of the average.\n",
    "    '''\n",
    "    test_data = pd.read_csv(\"https://raw.githubusercontent.com/eliiza/ml-training-data/master/housing_price_data/test_data.csv\")\n",
    "    actual_values = test_data['SalePrice']\n",
    "    # Pass in all columns except SalePrice\n",
    "    test_input = test_data.filter(regex='^(?!SalePrice$).*')\n",
    "    predicted_saleprice = model_fn(test_input)\n",
    "    mae = np.mean(np.abs(predicted_saleprice-actual_values))\n",
    "    if print_result:\n",
    "        return print(\"The model is inaccurate by $%.2f on average.\" % mae)\n",
    "    else:\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for data encoding\n",
    "\n",
    "# Step 1: Feature Engineering\n",
    "def encode_data(data,scaler = None):\n",
    "    \"\"\"\n",
    "    Encode a dataframe of house price data using the desired feature engineering process. \n",
    "    The scaler argument allows you to either scale the data anew (scaler = None), \n",
    "    or use previously derived scaling parameters\n",
    "    e.g. when you want to encoding test data using the scaling parameters from the training dataset.\n",
    "    Returns a dataframe of engineered features and the scaler object.\n",
    "    \"\"\"\n",
    "    \n",
    "    features = data.copy()\n",
    "    \n",
    "    # Numerical features\n",
    "    features = features[['OverallQual','GrLivArea','BedroomAbvGr','FullBath','YearBuilt']]\n",
    "    features['QualAreaInteract'] = features['OverallQual'] * features['GrLivArea']\n",
    "    \n",
    "    # Ordinal feature - map to numerical as before\n",
    "    cond_map = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1}\n",
    "    features['KitchenQual'] = data['KitchenQual'].map(cond_map).fillna(0)\n",
    "    \n",
    "    # Categorical features - one-hot encode using pre-written helper functions (below)\n",
    "    features['CentralAir'] = data['CentralAir'] == 'Y'\n",
    "    electrical = encode_electrical(data['Electrical'])\n",
    "    heating = encode_heating(data['Heating'])\n",
    "    features = pd.concat([features,electrical,heating],axis=1)\n",
    "    \n",
    "    # Convert to float data type for scaling process\n",
    "    features = features.astype(float)\n",
    "    \n",
    "    # Scale all the features\n",
    "    # If no `scaler` object in the function arguments - carry out scaling anew\n",
    "    # If `scaler` object in the function arguments - use those scaling parameters\n",
    "    if(not scaler):\n",
    "       scaler = MinMaxScaler()\n",
    "       scaler.fit(features)\n",
    "    features = pd.DataFrame(scaler.transform(features), \n",
    "                            columns = ['OverallQual','GrLivArea','BedroomAbvGr','FullBath','YearBuilt',\n",
    "                                       'QualAreaInteract','KitchenQual',\n",
    "                                       'CentralAir','FuseA','FuseF','FuseP','Mix','SBrkr',\n",
    "                                       'GasA','GasW','Grav','Wall'])\n",
    "    \n",
    "    # Return the desired data frame and the scaling parameters used\n",
    "    return(features,scaler)\n",
    "\n",
    "\n",
    "# Helper functions for one hot encoding\n",
    "\n",
    "def encode_electrical(electrical):\n",
    "    \"\"\"\n",
    "    Create data frame with one column per category in 'electrical' column, rows are Boolean with respect to\n",
    "    category string in electrical column.\n",
    "    \"\"\"\n",
    "    one_hot_encoding = pd.DataFrame()\n",
    "    one_hot_encoding['FuseA'] = electrical == 'FuseA'\n",
    "    one_hot_encoding['FuseF'] = electrical == 'FuseF'\n",
    "    one_hot_encoding['FuseP'] = electrical == 'FuseP'\n",
    "    one_hot_encoding['Mix']   = electrical == 'Mix'\n",
    "    one_hot_encoding['SBrkr'] = electrical == 'SBrkr'\n",
    "    return(one_hot_encoding)\n",
    "\n",
    "def encode_heating(heating):\n",
    "    \"\"\"\n",
    "    Create data frame with one column per category in 'heating' column, rows are Boolean with respect to\n",
    "    category string in heating column.\n",
    "    \"\"\"\n",
    "    one_hot_encoding = pd.DataFrame()\n",
    "    one_hot_encoding['GasA'] = heating == 'GasA'\n",
    "    one_hot_encoding['GasW'] = heating == 'GasW'\n",
    "    one_hot_encoding['Grav'] = heating == 'Grav'\n",
    "    one_hot_encoding['Wall'] = heating == 'Wall'\n",
    "    return(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training data\n",
    "training_set = pd.read_csv(\"https://raw.githubusercontent.com/eliiza/ml-training-data/master/housing_price_data/training_data.csv\")\n",
    "\n",
    "# Feature engineering as in previous notebook (using function in utils.py)\n",
    "training_features, training_scaler = encode_data(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Support Vector Regression hyperparameters\n",
    "\n",
    "How well do the default SVR hyperparameters perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is inaccurate by $51635.91 on average.\n"
     ]
    }
   ],
   "source": [
    "# SVR\n",
    "\n",
    "# Step 2: Train the model\n",
    "predictor = sklearn.svm.SVR(gamma='auto') \n",
    "predictor.fit(training_features, training_set['SalePrice'])\n",
    "\n",
    "# Step 3: Create a function that can make predictions using the model\n",
    "def svr_model(input_data, scaler=training_scaler):\n",
    "    input_features,_ = encode_data(input_data,scaler)\n",
    "    predictions = predictor.predict(input_features)\n",
    "    return(predictions)\n",
    "\n",
    "# Step 4: Evaluate\n",
    "evaluate_model(svr_model, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We will try to tune these hyperparameters:\n",
    "\n",
    "- `C` = margin size to apply no penalty\n",
    "- `epsilon` = penalty parameter to apply to errors\n",
    "- `kernel` = type of transformation applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chosen hyperparameter values for the Grid Search\n",
    "hyperparameters = {'C':[1E5, 1E6, 1E7], 'epsilon':[100, 250, 500], \n",
    "                   'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# Define the estimator to train\n",
    "svr = sklearn.svm.SVR(gamma='auto')\n",
    "\n",
    "# Define the Grid Search CV to undertake\n",
    "svr_grid_search = GridSearchCV(svr, hyperparameters, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=SVR(gamma='auto'),\n",
       "             param_grid={'C': [100000.0, 1000000.0, 10000000.0],\n",
       "                         'epsilon': [100, 250, 500],\n",
       "                         'kernel': ['linear', 'rbf']})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Grid Search Cross Validation process\n",
    "svr_grid_search.fit(training_features, training_set['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000000.0, 'epsilon': 100, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what hyperparameter combination was best\n",
    "svr_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuned_svr_model(input_data):\n",
    "    \"\"\"\n",
    "    Create a function that applies the tuned SVR model to test data.\n",
    "    Returns predictions for the 'SalesPrice' column.\n",
    "    \"\"\"\n",
    "    # Create scaler for use on test data\n",
    "    training_features, scaler = encode_data(training_set)\n",
    "    # Apply scaler for feature engineering on test data\n",
    "    input_features,_ = encode_data(input_data,scaler)\n",
    "    # Predict using the test data\n",
    "    predictions = svr_grid_search.best_estimator_.predict(input_features)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is inaccurate by $20804.21 on average.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the tuned model\n",
    "evaluate_model(tuned_svr_model, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
