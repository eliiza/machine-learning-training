{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering and multi-variable linear regression \n",
    "\n",
    "In this notebook we will be looking into multi-variable linear regression. The `LinearRegression` object that we created in the last lab remains the same, but by changing the features that we train it on we can improve its accuracy.\n",
    "\n",
    "The focus for this notebook then, is primarily about feature engineering and input pipelines. As a result we'll be making a wrapper to train and evaluate a model, given an input pipeline.\n",
    "\n",
    "But first, some boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_housing_data(test_size=0.2, random_state=None):\n",
    "    # Load data from Eliiza's github page\n",
    "    raw_data = pd.read_csv(\"https://raw.githubusercontent.com/eliiza/ml-training-data/master/housing_price_data/housing_data.csv\") \n",
    "\n",
    "    # Separate labels from feature columns.\n",
    "    X = raw_data.drop('SalePrice', axis=1)\n",
    "    y = raw_data['SalePrice']\n",
    "    \n",
    "    # Split the dataset with the requested proportions.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Return in standard order.\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_housing_data(random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model performance\n",
    "\n",
    "Depending on the features we feed in to the model, we will see different results. Throughout this notebook we'll call the following method to train and evaluate each model and input pipeline.\n",
    "\n",
    "This method will do the following:\n",
    "\n",
    "1. Load the dataset.\n",
    "1. \"Fit\" the input pipeline, and apply the transformation to the training data.\n",
    "1. Train the model on the training dataset.\n",
    "1. Feed the test data through the input pipeline.\n",
    "1. Make predictions on the processed test data.\n",
    "1. Calculate and return the Mean Absolute Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def train_and_evaluate_model(pipeline, model=LinearRegression()):\n",
    "    # Load the data\n",
    "    (X_train, y_train), (X_test, y_test) = load_housing_data(random_state=100)\n",
    "    \n",
    "    # Prepare the input pipeline and training data\n",
    "    X_train_prepared = pipeline.fit_transform(X_train)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_prepared, y_train)\n",
    "    \n",
    "    # Prepare the test data\n",
    "    X_test_processed = pipeline.transform(X_test)\n",
    "    \n",
    "    # Make some predictions\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    \n",
    "    # Calculate the error\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the dataset\n",
    "\n",
    "Next we will review the dataset once again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise/Discussion\n",
    "* Have a look at the count for `LotFrontage` - why is it lower and what should we do about it?\n",
    "* `MasVnrArea` is missing 6 out of 1168 values. What if that ratio is 1 in 2000 and the missing value was in the test set? Or production data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise if we look at a categorical column (not displayed above) we'll see that there are some missing values. What should we do with that value when training and making predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Electrical'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a new feature\n",
    "\n",
    "We'll start off by adding an extra feature without any empty values.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Add another feature to the model - `GrLivArea`. We'll use a `ColumnTransformer` which in its simplest form will filter out our selected columns.\n",
    "\n",
    "[ColumnTransformer Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_features = ['OverallQual']\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    ('numeric_pipeline', 'passthrough', numeric_features)\n",
    "])\n",
    "\n",
    "train_and_evaluate_model(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now try adding a feature with `null`/`NaN` values, such as `LotFrontage`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['OverallQual', 'GrLivArea']\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    ('numeric_pipeline', 'passthrough', numeric_features)\n",
    "])\n",
    "\n",
    "train_and_evaluate_model(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing values\n",
    "\n",
    "If a column is missing values, SciKit Learn provides an object called a `SimpleImputer` that is able to fill in missing values automatically, based on the training data and a user determined `strategy`. Possible strategies for numeric data are `median`, `mean`, or `constant` (with a constant value provided). For categorical types, `most_frequent` is generally a good choice, although `constant` will often work as well.\n",
    "\n",
    "The code below will replace any missing values with the median value of the training set.\n",
    "\n",
    "[SimpleImputer Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n",
    "\n",
    "**Note:** there is no harm in applying this to the other columns.<br>\n",
    "**Note2:** [Research](https://www.hilarispublisher.com/open-access/a-comparison-of-six-methods-for-missing-data-imputation-2155-6180-1000224.pdf) has shown, that supervised learning methods allow for superior imputations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_features = ['OverallQual', 'GrLivArea', 'LotFrontage']\n",
    "\n",
    "# Create a pipeline for numeric features\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('simple_imputer', SimpleImputer(strategy='median')),\n",
    "])\n",
    "\n",
    "# Pass only numeric features to the numeric pipeline. Later we will feed different features to different pipelines.\n",
    "pipeline = ColumnTransformer([\n",
    "    ('numeric_pipeline', numeric_pipeline, numeric_features)\n",
    "])\n",
    "\n",
    "train_and_evaluate_model(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise/Discussion\n",
    "\n",
    "There are three possible median values we could use to fill in the missing values: \n",
    "\n",
    "- The median of the training set\n",
    "- The median of the test\n",
    "- The median of the entire dataset.\n",
    "\n",
    "Which one should we use? Lets have a look, first get the values of each median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get NaN values from the test set.\n",
    "nan_entries = X_test[np.isnan(X_test.LotFrontage)]\n",
    "\n",
    "# Combine train and test sets\n",
    "X_all = pd.concat([X_train, X_test])\n",
    "\n",
    "# Fit the pipeline:\n",
    "pipeline.fit(X_train)\n",
    "\n",
    "# Print some stats:\n",
    "print(\"Train median:\", X_train.LotFrontage.median())\n",
    "print(\"Test median: \", X_test.LotFrontage.median())\n",
    "print(\"Comb. median:\", X_all.LotFrontage.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the next block, guess what value will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input:\")\n",
    "print(nan_entries[0:5][numeric_features])\n",
    "\n",
    "# Apply the transformation to the null entries:\n",
    "print(\"\\n\\nOutput:\")\n",
    "pipeline.transform(nan_entries[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling inputs\n",
    "\n",
    "It is common for ML algorithms to have a preference for scaled inputs - if numbers are very big with a big variance it is possible they will dominate the model in some way.\n",
    "\n",
    "### Exercise\n",
    "A benefit of using pipelines is that it is easy to chain together input transformations. Scikit Learn provides a StandardScaler class that will examine a dataset, and figure out how to transform it to make the mean 0, and standard deviation 1. try adding a `StandardScaler` to the pipeline below. \n",
    "\n",
    "- Does it make a difference to the model performance?\n",
    "- Does it matter if it goes before/after the imputer? Why/why not?\n",
    "\n",
    "[StandardScaler Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_features = ['OverallQual', 'GrLivArea', 'LotFrontage']\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('simple_imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    ('numeric_pipeline', numeric_pipeline, numeric_features)\n",
    "])\n",
    "\n",
    "train_and_evaluate_model(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the output look like?\n",
    "\n",
    "Let's take a look at the output data after the transformations above have been applied. The goal is to achieve the following properties:\n",
    "\n",
    "* No missing values - counts should all be equal.\n",
    "* Mean of ~0\n",
    "* Std Deviation of ~1\n",
    "\n",
    "The input data denerally won't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[numeric_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets apply our input pipeline and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipeline.fit(X_train[numeric_features])\n",
    "\n",
    "processed = numeric_pipeline.transform(X_test[numeric_features])\n",
    "\n",
    "pd.DataFrame(processed, columns=numeric_features).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables\n",
    "\n",
    "So far we have only looked at numerical values. But what do we do if we have a categorical feature. One example is the `CentralAir` column which has values of `Y` or `N`, indicating whether a property has Central Air Conditioning.\n",
    "\n",
    "To be useful in a linear regression model we need to convert this into a numeric value. This can be achieved in a number of ways, e.g. `'N' = 0` and `'Y' = 1`, you could also use `-1` and `1`. \n",
    "\n",
    "The most important detail is to apply the same transformation on the training data as the test and production data. Once again, `fit`-ing a pipeline can help ensure we achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "aircon_feature = ['CentralAir']\n",
    "\n",
    "aircon_encoder = OrdinalEncoder()\n",
    "aircon_encoder.fit(X_train[aircon_feature])\n",
    "\n",
    "print(aircon_encoder.categories_)\n",
    "\n",
    "aircon_encoder.transform(X_test[aircon_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we would add the `OrdinalEncoder` to our pipeline, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['CentralAir']\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    ('numerical', numeric_pipeline, numeric_features),\n",
    "    ('ordinal_encoder', OrdinalEncoder(), ordinal_features)\n",
    "])\n",
    "\n",
    "# Feed in unfiltered test data.\n",
    "train_and_evaluate_model(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal variables\n",
    "\n",
    "The example above used the `OrdinalEncoder` to convert from a category to a `0` or a `1`, which isn't a great example of an ordinal variable. A better example is the `BsmtCond` variable, which has a range of values based on the condition:\n",
    "\n",
    "    BsmtCond: Evaluates the general condition of the basement\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical - slight dampness allowed\n",
    "       Fa\tFair - dampness or some cracking or settling\n",
    "       Po\tPoor - Severe cracking, settling, or wetness\n",
    "\n",
    "As discussed in the slides there are two options to encode this, One Hot and Ordinal. In this specific case we want to ensure our model knows that these values have an order - that is: Excellent > Good > Typical etc. One hot encoding won't capture this information so an ordinal encoding is prefered.\n",
    "\n",
    "We also have to deal with situations where there is no basement, and `BsmtCond is NaN`. For this we will use a `SimpleImputer` as above.\n",
    "\n",
    "Another challenge is that we can't trust the `OrdinalEncoder` to figure out the correct order of the values, we need to specify this directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['BsmtCond'].astype('category').cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Not all categories are present. We will still include `Ex` since it is a valid value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordinal_features = [\n",
    "    'CentralAir',\n",
    "    'BsmtCond'\n",
    "]\n",
    "\n",
    "ordinal_categories = [\n",
    "    ['N', 'Y'],                     # CentralAir\n",
    "    ['Po', 'Fa', 'TA', 'Gd', 'Ex']  # BsmtCond\n",
    "]\n",
    "\n",
    "ordinal_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ('ordinal_encoder', OrdinalEncoder(categories=ordinal_categories))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the pipelines using a `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ColumnTransformer([\n",
    "    ('numeric_pipeline', numeric_pipeline, numeric_features),\n",
    "    ('ordinal_pipeline', ordinal_pipeline, ordinal_features)\n",
    "])\n",
    "\n",
    "train_and_evaluate_model(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Add KitchenQuality as a feature to the code above: see `KitchenQual` in [data_description.txt](https://github.com/eliiza/ml-training-data/blob/master/housing_price_data/data_description.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no order, we use a technique called 'one hot encoding'. This involves creating a new column for each category. This is useful because we specifically want to avoid our model assuming that there is some ordering to the data.\n",
    "\n",
    "For example, the `Electrical` variable in the housing prices dataset contains the following categories:\n",
    "\n",
    "       SBrkr\tStandard Circuit Breakers & Romex\n",
    "       FuseA\tFuse Box over 60 AMP and all Romex wiring (Average)\t\n",
    "       FuseF\t60 AMP Fuse Box and mostly Romex wiring (Fair)\n",
    "       FuseP\t60 AMP Fuse Box and mostly knob & tube wiring (poor)\n",
    "       Mix\tMixed\n",
    "\n",
    "Each row of our data would then have a 1 in the column corresponding to the value in the `Electrical`, and a 0 in all other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|FuseA |FuseF|FuseP|Mix|SBrkr| \n",
    "|-:-|-:-|-:-|-:-|-:-|\n",
    "| 1| 0 | 0 | 0 | 0 |\n",
    "| 0|1  | 0 | 0 | 0 |\n",
    "| 0|0  | 1 | 0 | 0 |\n",
    "| 0|0  | 0 | 1 | 0 |\n",
    "| 0|0  | 0 | 0 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_features = ['Electrical']\n",
    "\n",
    "one_hot_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ('one_hot_encoder', OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ColumnTransformer([\n",
    "    ('numeric_pipeline', numeric_pipeline, numeric_features),\n",
    "    ('ordinal_pipeline', ordinal_pipeline, ordinal_features),\n",
    "    ('one_hot_pipeline', one_hot_pipeline, one_hot_features)\n",
    "])\n",
    "\n",
    "train_and_evaluate_model(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exericse\n",
    "\n",
    "Try adding `Heating` and/or `Neighborhood` to the one hot encoding pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "combine_features = ['FullBath', 'BedroomAbvGr']\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    ('numeric_pipeline', numeric_pipeline, numeric_features),\n",
    "    ('ordinal_pipeline', ordinal_pipeline, ordinal_features),\n",
    "    ('one_hot_pipeline', one_hot_pipeline, one_hot_features),\n",
    "    ('combine_features', PolynomialFeatures(interaction_only=True), combine_features)\n",
    "])\n",
    "\n",
    "train_and_evaluate_model(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Exercise (optional)\n",
    "\n",
    "Build a linear model to achieve as low score as possible. See if you can get under $20,000 MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
