{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Models"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Vectorization\n",
        "* Data frames\n",
        "* Test Driven Development\n",
        "* Evaluating Regression:\n",
        "    * Mean absolute error\n",
        "    * Mean squared error\n",
        "    * Weighted Loss Functions\n",
        "    * R^2\n",
        "    * Residuals\n",
        "    * Max outlier\n",
        "* Divide into training and test sets.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using the following data science libraries (click the links for cheat sheets provided by DataCamp)\n",
        "* [numpy](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)  (for vectorised math operations)\n",
        "* [pandas](http://datacamp-community.s3.amazonaws.com/9f0f2ae1-8bd8-4302-a67b-e17f3059d9e8) (for dataframes)\n",
        "* [keras](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Keras_Cheat_Sheet_Python.pdf) (for neural networks)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step to training models is to figure out a way to tell how good a model is. This is similar to test driven development. Before we write our functions, we first define the tests that the functions need to pass. In the world of machine learning, these tests are defined by the data available.\n",
        "\n",
        "Note that unlike TDD, we don't need to pass all the tests. There will almost always be some difference between the\n",
        "\n",
        "It's considered good practice to split our data up into a \"training set\" (for inspection and training models), and a \"test set\" for model evaluation.\n",
        "\nThis is to ensure a fair test of the model's ability to generaliset to new examples. The same reason why an exam contains different questions to the practice exams a student learns from."
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labelled = pd.read_csv(\"housing_price_data/train.csv\")"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the size of our data\n",
        "labelled.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": [
              "(1460, 81)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split up into training and test sets.\n",
        "num_rows = labelled.shape[0]\n",
        "training_set = labelled[:round(num_rows*0.8)]\n",
        "test_set = labelled[round(num_rows*0.8):]\n",
        "training_set.to_csv(\"housing_price_data/training_data.csv\")\n",
        "test_set.to_csv(\"housing_price_data/test_data.csv\")"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now write a function to evaluate how accurate any given predictive model is at predicting on our test set.\n",
        "\n",
        "We'll use the mean absolute error. Defined as\n",
        "\n$$  \\frac{1}{n} * \\sum_{i=1}^n |\\mathsf{predicted\\_value} - \\mathsf{actual\\_value}|  $$"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}