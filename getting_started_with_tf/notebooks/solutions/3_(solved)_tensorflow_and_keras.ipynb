{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Tensorflow and Keras\n",
    "\n",
    "In the beginning these were separate projects, TensorFlow provided a framework to parallelise computations on one or more GPUs. Despite not being directly made for ML, many of the building blocks were very well suited. As a result, Keras sprung up as a set of ML building blocks built upon TensorFlow. Over time the two became almost synonymous, to the point that now Keras is a core module within TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "# Reshape the labels.\n",
    "train_labels = train_labels[:,0]\n",
    "test_labels = test_labels[:,0]\n",
    "\n",
    "# And scale.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras provides neural network Lego blocks\n",
    "\n",
    "We have different types/catergories of bricks and we stack them on top of one another to make something useful (or cool). Roughly speaking we can think of the following types when building Neural Networks, where every Network we build should have a:\n",
    " - An architecture\n",
    " - Loss function\n",
    " - Optimizer\n",
    "\n",
    "Keras/Tensorflow offers a bunch of these built-in. But sometimes we'd need a lego brick very specific to our use case so we'd have to build it by ourselves from scratch, and Keras gives us the ability to do that.\n",
    "\n",
    "## Model Building Blocks\n",
    "\n",
    "`tensorflow.keras.layers` ([Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers)):\n",
    "* `Dense` - A fully connected layer of neurons.\n",
    "* `Conv2D` - A convolution layer\n",
    "* `MaxPooling2D` - Subsampling/shrinking layer.\n",
    "* `Dropout` - Regularisation technique.\n",
    "* `BatchNormalisation` - Normalisation of layer output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32, 3)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "`tensorflow.keras.losses` contains prebuilt loss functions ([Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses)):\n",
    "\n",
    "* `MeanSquaredError`\n",
    "* `MeanAbsoluteError`\n",
    "* `SparseCategoricalCrossentropy`\n",
    "* 15 or so more options.\n",
    "\n",
    "It is also possible to build and use your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisers\n",
    "\n",
    "`tensorflow.keras.optimizers` provides a number of optimisers ([Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)):\n",
    "\n",
    "* `SGD` - Stochastic Gradient Descent\n",
    "* `Adam`\n",
    "* `Adagrad` - As well as other `Ada*` optimisers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "`tensorflow.keras.metrics` provides a number of metrics that can be displayed when training - they do not actually have any impact on the training though ([Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)).\n",
    "\n",
    "* `Accuracy`\n",
    "* `Precision`\n",
    "* `Recall`\n",
    "* `CategoricalAccuracy`\n",
    "* `SparseCategoricalAccuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Using standard TensorFlow components to preprocess data.\n",
    "\n",
    "Image augmentation is another technique to expand your training data by modifying images. For example, flipping, scaling and rotating an image of a cat shouldn't have any impact on the models ability to classify it as a dog.\n",
    "\n",
    "If we only had 10,000 images we can artificially expand the dataset.\n",
    "\n",
    "See [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_gen = ImageDataGenerator(horizontal_flip=True, \n",
    "                               zoom_range=0.15,\n",
    "                               width_shift_range=0.1,\n",
    "                               height_shift_range=0.1,\n",
    "                               rotation_range=15)\n",
    "\n",
    "#Pretend we only have 10,000 images.\n",
    "train_data_gen = image_gen.flow(train_images[:10000], train_labels[:10000])\n",
    "\n",
    "# ...build a model\n",
    "#\n",
    "#\n",
    "# Note how the data is passed to the model.fit(...) call.\n",
    "# model.fit(train_data_gen,\n",
    "#          epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(0,5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(augmented_images[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Callbacks\n",
    "\n",
    "As per the pattern, `tensorflow.keras.callbacks` provides a set of useful callbacks ([Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks)):\n",
    "* `EarlyStopping` - Stop training early if loss improvement is less than a certain amount.\n",
    "* `LearningRateScheduler` - A callback to adjust the learning rate over time.\n",
    "* `ModelCheckpoint` - Allows models to be saved periodically.\n",
    "\n",
    "It is also possible to create your own callbacks to run at the start/end of training/test epochs/batches. (And other permutations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Try using a different optimiser, for example Stochastic Gradient Descent (SGD). Is the performance better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32, 3)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Use SGD instead of Adam:\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=[accuracy])\n",
    "\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "The learning rate can be tuned to improve model performance. Try training and evaluating the same model with a learning rate of `0.0001` and `0.01`. Read [docs](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) to see how. (The default value is `0.001`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [0.01, 0.0001]:\n",
    "\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(32, 32, 3)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    # Set new learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=[accuracy])\n",
    "\n",
    "    model.fit(train_images, train_labels,\n",
    "              epochs=10)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "    print(f\"Test accuracy for learning rate of {lr}: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Imagine we only have 5,000 images, try training on this set and see how the accuracy varies between the training and test set.\n",
    "\n",
    "* Why do the epochs take less time to train?\n",
    "* What happes if we let it run for more epochs?\n",
    "* How does the training accuracy compare to the test accuracy?\n",
    "* Try augmenting the dataset using an `ImageDataGenerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each epoch takes less time to train as there is less training data\n",
    "# (an epoch is a single run through the training data)\n",
    "\n",
    "# If we let this code run for more epochs, training accuracy will increase,\n",
    "# but the risk of overfitting becomes greater\n",
    "\n",
    "# If we run this training procedure for 50 epochs, we find that \n",
    "# the training accuracy is significantly higher than the test accuracy.\n",
    "# This is a sign that we may be overfitting to the training data.\n",
    "\n",
    "small_train_images = train_images[0:5000]\n",
    "small_train_labels = train_labels[0:5000]\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32, 3)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=[accuracy])\n",
    "\n",
    "model.fit(small_train_images, small_train_labels,\n",
    "          epochs=50)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can avoid overfitting by creating a larger \"effective\" training\n",
    "# dataset with ImageDataGenerator\n",
    "\n",
    "small_train_images = train_images[0:5000]\n",
    "small_train_labels = train_labels[0:5000]\n",
    "\n",
    "image_gen = ImageDataGenerator(horizontal_flip=True, \n",
    "                               zoom_range=0.15,\n",
    "                               width_shift_range=0.1,\n",
    "                               height_shift_range=0.1,\n",
    "                               rotation_range=15)\n",
    "train_data_gen = image_gen.flow(small_train_images, small_train_labels)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32, 3)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=[accuracy])\n",
    "\n",
    "model.fit(train_data_gen, epochs=50)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Have a look at the code below, it introduces custom callbacks and early stopping. Try to understand what is happening and run it to see if the output matches your expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32, 3)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# This callback will print a message when each epoch begins and ends\n",
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_begin(self, epoch, logs=None):\n",
    "    print('Starting epoch {}!!!'.format(epoch+1))\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nFinished epoch {}!!!'.format(epoch+1))\n",
    "\n",
    "# This callback will cause training to stop if loss doesn't decrease by at\n",
    "# least 0.1 across 3 consecutive epochs\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.1, patience=3, verbose=1)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=[accuracy])\n",
    "\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=100,\n",
    "          callbacks=[MyCustomCallback(), early_stop])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "\n",
    "* Try using a `LearningRateScheduler` to decrease the learning rate over time.\n",
    "* Try classifying the same image set to distinguish between binary categories, e.g. cat vs not-cat, animal vs machine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
