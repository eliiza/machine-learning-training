{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SciKit API\n",
    "\n",
    "\n",
    "We now have a sense of the workflow necessary to use machine learning models.\n",
    "\n",
    "The steps we take are:\n",
    "\n",
    "1. _feature engineering_: Encode all the data we want to use into numbers. Perform any other transformations you think are necessary (e.g. scaling). We end up with a 2D array `X_train` containing all the columns we want to process, as well as an array `y_train` containing the column we want to predict (possibly scaled).\n",
    "\n",
    "2. _train our model_:  This can be done with the command `model.fit(X_train,y_train)`.\n",
    "\n",
    "3. _predict_: Use our freshly trained model to make some predictions.  This is done with the command `model.predict(input)`.\n",
    "\n",
    "4. _evaluate_: We can use the `evaluate_model` function.\n",
    "\n",
    "We will now apply this workflow to different algorithms from the SciKit Learn toolkit: Gradient Boosting, Random Forests and Support Vector Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_housing_data(test_size=0.2, random_state=None):\n",
    "    # Load data from Eliiza's github page\n",
    "    raw_data = pd.read_csv(\"https://raw.githubusercontent.com/eliiza/ml-training-data/master/housing_price_data/housing_data.csv\") \n",
    "\n",
    "    # Separate labels from feature columns.\n",
    "    X = raw_data.drop('SalePrice', axis=1)\n",
    "    y = raw_data['SalePrice']\n",
    "    \n",
    "    # Split the dataset with the requested proportions.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Return in standard order.\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_housing_data(random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def train_and_evaluate_model(pipeline, model=LinearRegression()):\n",
    "    # Load the data\n",
    "    (X_train, y_train), (X_test, y_test) = load_housing_data(random_state=100)\n",
    "    \n",
    "    # Prepare the input pipeline and training data\n",
    "    X_train_prepared = pipeline.fit_transform(X_train)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_prepared, y_train)\n",
    "    \n",
    "    # Prepare the test data\n",
    "    X_test_processed = pipeline.transform(X_test)\n",
    "    \n",
    "    # Make some predictions\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    \n",
    "    # Calculate the error\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Feature Engineering\n",
    "\n",
    "We are going to use some of the features we looked at in previous notebooks, initially:\n",
    "\n",
    "- Numeric:\n",
    "    - `OverallQual`\n",
    "    - `GrLivArea`\n",
    "    - `LotFrontage`\n",
    "    - `LotArea`\n",
    "    - `YearBuilt`\n",
    "    - `MSSubClass`\n",
    "    - `OverallCond`\n",
    "- Categorical (to be one-hot encoded):\n",
    "    - `Electrical`\n",
    "    - `Heating`\n",
    "    - `Neighborhood`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Review the code below - we are simply making a function to do the feature engineering steps in a repeatable manner.\n",
    "- Add any missing features that you found helped improve accuracy in the previous notebook. **For example ordinal variable(s).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Numeric features:\n",
    "numeric_features = ['OverallQual', 'GrLivArea', 'LotFrontage', 'LotArea', 'YearBuilt', 'MSSubClass', 'OverallCond']\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('simple_imputer', SimpleImputer(strategy='median')),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical features:\n",
    "one_hot_features = ['Electrical', 'Heating', 'Neighborhood']\n",
    "\n",
    "one_hot_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ('one_hot_encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Combine into single pipeline:\n",
    "pipeline = ColumnTransformer([\n",
    "    ('numeric_pipeline', numeric_pipeline, numeric_features),\n",
    "    ('one_hot_pipeline', one_hot_pipeline, one_hot_features)\n",
    "])\n",
    "\n",
    "train_and_evaluate_model(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Gradient Boosting\n",
    "\n",
    "We will be using SciKit Learn's Gradient Boosting implementation: [sklearn.ensemble.GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html). There are other implementations of all of these algorithms which we won't use here, including [XGBoost](https://xgboost.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "train_and_evaluate_model(pipeline, GradientBoostingRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does this accuracy compare with your previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This disciplined workflow makes it easy to try other models.  \n",
    "This [cheat sheet](https://www.analyticsvidhya.com/infographics/Scikit-Learn-Infographic.pdf) provides a list of models supported by scikit learn.  \n",
    "Let's try two more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Errors\n",
    "\n",
    "- Explore the predictions and errors in this Gradient Boosting model. \n",
    "- Are the outliers the same as in the previous models you've fitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Random Forest\n",
    "\n",
    "Implement a model using the same workflow as above with [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Hint: to avoid a warning regarding argument defaults, set `n_estimators = 100` within `RandomForestRegressor()`.\n",
    "\n",
    "How does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Support Vector Regression\n",
    "\n",
    "Implement a model using [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). Hint: to avoid a warning regarding argument defaults, set `gamma='auto'` within `SVR()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this performance surprising? Have a quick look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) - any ideas how you might improve its performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Algorithm hyperparameters (optional)\n",
    "\n",
    "Each of these algorithms has various hyperparameters that can be adjusted to fine tune the model.\n",
    "\n",
    "Hyperparameters are the options used to direct the learning process. Parameters are the learned model coefficients.\n",
    "\n",
    "- Gradient Boosting\n",
    "    - Main tuning hyperparameters:\n",
    "         - `n_estimators`\n",
    "         - `learning_rate`\n",
    "         - `max_depth`\n",
    "- Random Forests\n",
    "    - Main tuning hyperparameters:\n",
    "         - `n_estimators`\n",
    "         - `max_features`\n",
    "         - `max_depth`\n",
    "- Support Vector Regression\n",
    "    - Main tuning hyperparameters:\n",
    "        - `C`\n",
    "        - `epsilon`\n",
    "        - `kernel`\n",
    "        \n",
    "        \n",
    "- Different algorithms respond to hyperparameter adjustments differently. For this data problem, the performance of the SVR model is *highly dependent* on the values of the hyperparameters. This is what can improve the SVR model to closer to the accuracy of the others.\n",
    "\n",
    "\n",
    "- Have a look at the documentation to see what each of these parameters do.\n",
    "- What are their default values?\n",
    "- Play around with changing these hyperparameters to see if you can improve the model accuracies.\n",
    "- For the SVR model - try much larger values for `epsilon` and `C`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
