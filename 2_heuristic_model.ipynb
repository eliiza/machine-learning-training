{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating models"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using the following data science libraries (click the links for cheat sheets provided by DataCamp)\n",
        "* [numpy](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)  (for vectorised math operations)\n",
        "* [pandas](http://datacamp-community.s3.amazonaws.com/9f0f2ae1-8bd8-4302-a67b-e17f3059d9e8) (for dataframes)\n",
        "* [keras](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Keras_Cheat_Sheet_Python.pdf) (for neural networks)\n",
        "* [scikitlearn](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf) (for other machine learning models)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import sklearn"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step to training models is to figure out a way to tell how good a model is. This is similar to test driven development. Before we write our functions, we first define the tests that the functions need to pass. In the world of machine learning, these tests are defined by the data available.\n",
        "\n",
        "Note that unlike TDD, we don't need to pass all the tests. There will almost always be some difference between the predicted outcome and the measured outcomes.\n",
        "\n",
        "It's considered good practice to split our data up into a \"training set\" (for inspection and training models), and a \"test set\" for model evaluation.\n",
        "\n",
        "This is to ensure a fair test of the model's ability to generalise to new examples. The same reason why an exam contains different questions to the practice exams a student learns from.\n",
        "\n",
        "Throughout this module, we'll be using the [Ames Housing Prices Data Set](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n",
        "\nThe data is described in `housing_price_data/data_description.txt`"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labelled = pd.read_csv(\"housing_price_data/train.csv\") # Load data\n",
        "# Split up into training and test sets.\n",
        "num_rows = labelled.shape[0]\n",
        "training_set = labelled[:round(num_rows*0.8)]\n",
        "test_set = labelled[round(num_rows*0.8):]\n",
        "training_set.to_csv(\"housing_price_data/training_data.csv\",index = False)\n",
        "test_set.to_csv(\"housing_price_data/test_data.csv\",index = False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Absolute Error\n",
        "Now that we have a test set, we can evaluate our models!\n",
        "\n",
        "We'll use the mean absolute error. This is the average size of the difference between the predicted value vs the observed value.\n",
        "\n",
        "Formally, this is defined as\n",
        "\n",
        "$$  \\mathsf{MAE} = \\frac{1}{n} * \\sum_{i=1}^n |\\mathsf{predicted\\_value}[i] - \\mathsf{actual\\_value}[i]|  $$\n",
        "\n\n",
        "We now write a function to evaluate how accurate any given predictive model is at predicting on our test set.\n"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model_fn):\n",
        "    '''\n",
        "    Consumes a function model_fn\n",
        "    and evaluates its predictive accuracy against \n",
        "    the housing prices test set.\n",
        "    '''\n",
        "    test_data = pd.read_csv(\"housing_price_data/test_data.csv\")\n",
        "    actual_values = test_data['SalePrice']\n",
        "    test_input = test_data.filter(regex='^(?!SalePrice$).*') #Pass in all columns except SalePrice\n",
        "    predicted_saleprice = model_fn(test_input)\n",
        "    mae = np.mean(np.abs(predicted_saleprice-actual_values))\n",
        "    print(\"The model is inaccurate by $%.2f on average.\" % mae)\n",
        "    return mae"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate a very simple predictive heuristic: Sale Price = 100,000 * number of bedrooms."
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bedroom_heuristic(input_data):\n",
        "    bedrooms = input_data['BedroomAbvGr']\n",
        "    prediction = 100000*bedrooms\n",
        "    return(prediction)\n",
        "\nevaluate_model(bedroom_heuristic)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: Try and make a heuristic that achieves a lower score.\n",
        "\nExercise: Extend `evaluate_model` to report how fast the model takes to make its predictions."
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.8.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}