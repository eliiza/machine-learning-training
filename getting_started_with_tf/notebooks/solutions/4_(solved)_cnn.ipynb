{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since convolutional neural networks have been proved to perform better than regular neural networks on images we can add convolution and pooling to the original neural network we built and trained on the CIFAR-10 dataset. Remember this achieved x% accuracy with the regular neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries and preprocess the data (as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# loading dataset\n",
    "cifar10 = keras.datasets.cifar10\n",
    "\n",
    "# loading train and test set\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "# flatten labels\n",
    "train_labels = train_labels.reshape(train_labels.shape[0])\n",
    "test_labels = test_labels.reshape(test_labels.shape[0])\n",
    "\n",
    "# declare class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# scale images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN model\n",
    "\n",
    "Building the convolutional neural network requires configuring the layers of the model, then compiling the model - just like the first simple neural network we built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the layers\n",
    "\n",
    "These are the layers we have in the first model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.Sequential([\n",
    "#         keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "#         keras.layers.Dense(128, activation='relu'),\n",
    "#         keras.layers.Dense(10, activation='softmax')\n",
    "#         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will simply add more layers to this simple architecture to turn it into a convolutional neural network.\n",
    "\n",
    "Based on the VGG16 paper, which can be found here https://neurohive.io/en/popular-networks/vgg16/, we will build a VGG network consisting of 3 blocks.\n",
    "\n",
    "Each of the 3 blocks will consist of 2 convolutional layers and a max pooling layer.\n",
    "\n",
    "Remember:\n",
    "\n",
    "* *Convolution*: dot product of matrix of pixels and filter matric (kernel)\n",
    "* *Max Pooling*: the outputs from convolution are pooled to reduce the size and therefore speed up computation. This is usually done by only selecting the maximum value of a section of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # block 1\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # block 2\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # block 3\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # old blocks - fully connected\n",
    "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the results on the train dataset\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great train and test accuracy. But like in the regular neural network there is a big difference between train accuracy and test accuracy ~15%. This difference indicates we have overfit the model for the training set which does not generalise well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid overfitting with dropout regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since there is a clear indication of overfitting let's try a technique to help avoid this. Dropout is a regularisation method where a certain number of layer outputs are randomly ignored, i.e. droppped out, forcing nodes within a layer to probabilistically take on more on less responsibility for the input.\n",
    " \n",
    "More can be read on dropout here: http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # block 1\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # dropout: 25% of outputs are randomly not passed to the next layer\n",
    "    keras.layers.Dropout(0.25),\n",
    "    \n",
    "    # block 2\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # dropout: 25% of outputs are randomly not passed to the next layer\n",
    "    keras.layers.Dropout(0.25),\n",
    "    \n",
    "    # block 3\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # dropout: 40% of outputs are randomly not passed to the next layer\n",
    "    keras.layers.Dropout(0.4),\n",
    "    \n",
    "    # old blocks - fully connected\n",
    "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the train accuracy for the model with dropout layers is lower than previously, the test accuracy is higher at 78%. This means that the model with dropout layers generalises better to unseen data. Now it's time to build one for yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more CNN blocks to model 1 and see what impact this has on the models performance. Feel free to play around with the number of training epochs too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # block 1\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # block 2\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # block 3\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    ###\n",
    "    #add more blocks here\n",
    "    ###\n",
    "    \n",
    "    # old blocks - fully connected\n",
    "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the model you built in Part 1 and add some dropout layers to see if this has an impact on overfitting and accuracy. Feel free to play around with dropout rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If time permits, add some data augmentation you learned in the Keras notebook to see if this has an impact on the performance of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
